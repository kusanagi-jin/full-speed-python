{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kusanagi-jin/full-speed-python/blob/master/prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mYpvAsXOX97a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# colab\n",
        "\n",
        "for gpu\n",
        "\n",
        "## Enabling GPU\n",
        "What's really amazing about collaboratory (or Google's generousity) is that there's also GPU option available.\n",
        "\n",
        "Follow on the collaboratory menu tabs, \"Runtime\" => \"Change runtime type\".\n",
        "\n",
        "<img src=\"http://deeplearnphysics.org/Blog/imgs/2018-03-02-Collab-00-img00.png\" width=\"50%\">\n",
        "\n",
        "### Choosing Runtime type\n",
        "Then you should see a pop-up where you can choose GPU.\n",
        "\n",
        "<img src=\"http://deeplearnphysics.org/Blog/imgs/2018-03-02-Collab-00-img01.png\" width=\"30%\"> run\n",
        "\n",
        "After you change your runtime, your runtime should automatically restart (which means information from executed cells disappear). ma\n",
        "\n",
        "### nvidia-smi\n",
        "If you own GPU you may be familiar with `nvidia-smi`, NVIDIA binary to print out gpu's utilization summary.\n",
        "\n",
        "`nvidia-smi` is not available by default but it is just a symbolic link away"
      ]
    },
    {
      "metadata": {
        "id": "SWdGMU65XZtK",
        "colab_type": "code",
        "outputId": "cb1b99e3-2771-4417-e5ff-7146a7218d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\r\n",
            "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
            "  Using cached https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.3)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.5)\n",
            "Collecting humanize\n",
            "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
            "  Using cached https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.6 GB  I Proc size: 141.6 MB\n",
            "GPU RAM Free: 11439MB | Used: 0MB | Util   0% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BXyU1Jzvao__",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## free ram \n",
        "if ram is not usuable more than 512 then restart it "
      ]
    },
    {
      "metadata": {
        "id": "GsJ5YIJYa3HT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pkill -9 -f ipykernel_launcher\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}